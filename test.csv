text,label
Avoiding Side Effects in Complex Environments Reward function specification can be difficult. Rewarding the agent for making a widget may be easy but penalizing the multitude of possible negative side effects is hard. In toy environments Attainable Utility Preservation AUP avoided side effects by penalizing shifts in the ability to achieve randomly generated goals. We scale this approach to large randomly generated environments based on Conway s Game of Life. By preserving optimal value for a single randomly generated reward function AUP incurs modest overhead while leading the agent to complete the specified task and avoid many side effects. Videos and code are available at,0
One for All Simultaneous Metric and Preference Learning over Multiple Users This paper investigates simultaneous preference and metric learning from a crowd of respondents. A set of items represented by d dimensional feature vectors and paired comparisons of the form item i is preferable to item j made by each user is given. Our model jointly learns a distance metric that characterizes the crowd s general measure of item similarities along with a latent ideal point for each user reflecting their individual preferences. This model has the flexibility to capture individual preferences while enjoying a metric learning sample cost that is amortized over the crowd. We first study this problem in a noiseless continuous response setting i.e. responses equal to differences of item distances to understand the fundamental limits of learning. Next we establish prediction error guarantees for noisy binary measurements such as may be collected from human respondents and show how the sample complexity improves when the underlying metric is low rank. Finally we establish recovery guarantees under assumptions on the response distribution. We demonstrate the performance of our model on both simulated data and on a dataset of color preference judgements across a large number of users.,0
Formalizing the Problem of Side Effect Regularization AI objectives are often hard to specify properly. Some approaches tackle this problem by regularizing the AI s side effects Agents must weigh off how much of a mess they make with an imperfectly specified proxy objective. We propose a formal criterion for side effect regularization via the assistance game framework. In these games the agent solves a partially observable Markov decision process POMDP representing its uncertainty about the objective function it should optimize. We consider the setting where the true objective is revealed to the agent at a later time step. We show that this POMDP is solved by trading off the proxy reward with the agent s ability to achieve a range of future tasks. We empirically demonstrate the reasonableness of our problem formalization via ground truth evaluation in two gridworld environments.,0
Discriminator Actor Critic Addressing Sample Inefficiency and Reward Bias in Adversarial Imitation Learning We identify two issues with the family of algorithms based on the Adversarial Imitation Learning framework. The first problem is implicit bias present in the reward functions used in these algorithms. While these biases might work well for some environments they can also lead to sub optimal behavior in others. Secondly even though these algorithms can learn from few expert demonstrations they require a prohibitively large number of interactions with the environment in order to imitate the expert for many real world applications. In order to address these issues we propose a new algorithm called Discriminator Actor Critic that uses off policy Reinforcement Learning to reduce policy environment interaction sample complexity by an average factor of . Furthermore since our reward function is designed to be unbiased we can apply our algorithm to many problems without making any task specific adjustments.,0
Reward Tampering Problems and Solutions in Reinforcement Learning A Causal Influence Diagram Perspective Can humans get arbitrarily capable reinforcement learning RL agents to do their bidding Or will sufficiently capable RL agents always find ways to bypass their intended objectives by shortcutting their reward signal This question impacts how far RL can be scaled and whether alternative paradigms must be developed in order to build safe artificial general intelligence. In this paper we study when an RL agent has an instrumental goal to tamper with its reward process and describe design principles that prevent instrumental goals for two different types of reward tampering reward function tampering and RF input tampering . Combined the design principles can prevent both types of reward tampering from being instrumental goals. The analysis benefits from causal influence diagrams to provide intuitive yet precise formalizations.,0
Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback We apply preference modeling and reinforcement learning from human feedback RLHF to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training where preference models and RL policies are updated on a weekly cadence with fresh human feedback data efficiently improving our datasets and models. Finally we investigate the robustness of RLHF training and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results we perform peripheral analyses on calibration competing objectives and the use of OOD detection compare our models with human writers and provide samples from our models using prompts appearing in recent related work.,0
TruthfulQA Measuring How Models Mimic Human Falsehoods We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises questions that span categories including health law finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well models must avoid generating false answers learned from imitating human texts. We tested GPT GPT Neo J GPT and a T based model. The best model was truthful on of questions while human performance was . Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks where performance improves with model size. However this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine tuning using training objectives other than imitation of text from the web.,0
Truthful AI Developing and governing AI that does not lie In many contexts lying the use of verbal falsehoods to deceive is harmful. While lying has traditionally been a human affair AI systems that make sophisticated verbal statements are becoming increasingly prevalent. This raises the question of how we should limit the harm caused by AI lies i.e. falsehoods that are actively selected for . Human truthfulness is governed by social norms and by laws against defamation perjury and fraud . Differences between AI and humans present an opportunity to have more precise standards of truthfulness for AI and to have these standards rise over time. This could provide significant benefits to public epistemics and the economy and mitigate risks of worst case AI futures.,0
BackdoorBench A Comprehensive Benchmark of Backdoor Learning Backdoor learning is an emerging and important topic of studying the vulnerability of deep neural networks DNNs . Many pioneering backdoor attack and defense methods are being proposed successively or concurrently in the status of a rapid arms race. However we find that the evaluations of new methods are often unthorough to verify their claims and real performance mainly due to the rapid development diverse settings as well as the difficulties of implementation and reproducibility. Without thorough evaluations and comparisons it is difficult to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma we build a comprehensive benchmark of backdoor learning called BackdoorBench. It consists of an extensible modular based codebase currently including implementations of state of the art SOTA attack and SOTA defense algorithms as well as a standardized protocol of a complete backdoor learning. We also provide comprehensive evaluations of every pair of attacks against defenses with poisoning ratios based on models and datasets thus pairs of evaluations in total. We further present analysis from different perspectives about these evaluations studying the effects of attack against defense algorithms poisoning ratio model and dataset in backdoor learning. All codes and evaluations of BackdoorBench are publicly available at url ,1
Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning Deep learning models have achieved high performance on many tasks and thus have been applied to many security critical scenarios. For example deep learning based face recognition systems have been used to authenticate users to access many security sensitive applications like payment apps. Such usages of deep learning systems provide the adversaries with sufficient incentives to perform attacks against these systems for their adversarial purposes. In this work we consider a new type of attacks called backdoor attacks where the attacker s goal is to create a backdoor into a learning based authentication system so that he can easily circumvent the system by leveraging the backdoor. Specifically the adversary aims at creating backdoor instances so that the victim learning system will be misled to classify the backdoor instances as a target label specified by the adversary. In particular we study backdoor poisoning attacks which achieve backdoor attacks using poisoning strategies. Different from all existing work our studied poisoning strategies can apply under a very weak threat model the adversary has no knowledge of the model and the training set used by the victim system the attacker is allowed to inject only a small amount of poisoning samples the backdoor key is hard to notice even by human beings to achieve stealthiness. We conduct evaluation to demonstrate that a backdoor adversary can inject only around poisoning samples while achieving an attack success rate of above . We are also the first work to show that a data poisoning attack can create physically implementable backdoors without touching the training process. Our work demonstrates that backdoor poisoning attacks pose real threats to a learning system and thus highlights the importance of further investigation and proposing defense strategies against them.,1
Locating and Editing Factual Associations in GPT We analyze the storage and recall of factual associations in autoregressive transformer language models finding evidence that these associations correspond to localized directly editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model s factual predictions. This reveals a distinct set of steps in middle layer feed forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall we modify feed forward weights to update specific factual associations using Rank One Model Editing ROME . We find that ROME is effective on a standard zero shot relation extraction zsRE model editing task comparable to existing methods. To perform a more sensitive evaluation we also evaluate ROME on a new dataset of counterfactual assertions on which it simultaneously maintains both specificity and generalization whereas other methods sacrifice one or another. Our results confirm an important role for mid layer feed forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code dataset visualizations and an interactive demo notebook are available at,1
Posterior calibration and exploratory analysis for natural language processing models Many models in natural language processing define probabilistic distributions over linguistic structures. We argue that the quality of a model s posterior distribution can and should be directly evaluated as to whether probabilities correspond to empirical frequencies and NLP uncertainty can be projected not only to pipeline components but also to exploratory data analysis telling a user when to trust and not trust the NLP analysis. We present a method to analyze calibration and apply it to compare the miscalibration of several commonly used models. We also contribute a coreference sampling algorithm that can create confidence intervals for a political event extraction task.,1
Interpretable Explanations of Black Boxes by Meaningful Perturbation As machine learning algorithms are increasingly applied to high impact yet high risk tasks such as medical diagnosis or autonomous driving it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years a number of image saliency methods have been developed to summarize where highly complex neural networks look in an image for evidence for their predictions. However these techniques are limited by their heuristic nature and architectural constraints. In this paper we make two main contributions First we propose a general framework for learning different kinds of explanations for any black box algorithm. Second we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works our method is model agnostic and testable because it is grounded in explicit and interpretable image perturbations.,1
STRIP A Defence Against Trojan Attacks on Deep Neural Networks A recent trojan attack on deep neural network DNN models is one insidious variant of data poisoning attacks. Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to misclassify any inputs signed with the attacker s chosen trojan trigger. Since the trojan trigger is a secret guarded and exploited by the attacker detecting such trojan inputs is a challenge especially at run time when models are in active operation. This work builds STRong Intentional Perturbation STRIP based run time trojan attack detection system and focuses on vision system. We intentionally perturb the incoming input for instance by superimposing various image patterns and observe the randomness of predicted classes for perturbed inputs from a given deployed model malicious or benign. A low entropy in predicted classes violates the input dependence property of a benign model and implies the presence of a malicious input a characteristic of a trojaned input. The high efficacy of our method is validated through case studies on three popular and contrasting datasets MNIST CIFAR and GTSRB. We achieve an overall false acceptance rate FAR of less than given a preset false rejection rate FRR of for different types of triggers. Using CIFAR and GTSRB we have empirically achieved result of for both FRR and FAR. We have also evaluated STRIP robustness against a number of trojan attack variants and adaptive attacks.,1
Network Dissection Quantifying Interpretability of Deep Visual Representations We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects parts scenes textures materials and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self supervised training tasks. We further analyze the effect of training iterations compare networks trained with different initializations examine the impact of network depth and width and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.,1
Can Backdoor Attacks Survive Time Varying Models  Backdoors are powerful attacks against deep neural networks DNNs . By poisoning training data attackers can inject hidden rules backdoors into DNNs which only activate on inputs containing attack specific triggers. While existing work has studied backdoor attacks on a variety of DNN models they only consider static models which remain unchanged after initial deployment.,1
Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles Deep neural networks NNs are powerful black box predictors that have recently achieved impressive performance on a wide spectrum of tasks. Quantifying predictive uncertainty in NNs is a challenging and yet unsolved problem. Bayesian NNs which learn a distribution over weights are currently the state of the art for estimating predictive uncertainty however these require significant modifications to the training procedure and are computationally expensive compared to standard non Bayesian NNs. We propose an alternative to Bayesian NNs that is simple to implement readily parallelizable requires very little hyperparameter tuning and yields high quality predictive uncertainty estimates. Through a series of experiments on classification and regression benchmarks we demonstrate that our method produces well calibrated uncertainty estimates which are as good or better than approximate Bayesian NNs. To assess robustness to dataset shift we evaluate the predictive uncertainty on test examples from known and unknown distributions and show that our method is able to express higher uncertainty on out of distribution examples. We demonstrate the scalability of our method by evaluating predictive uncertainty estimates on ImageNet.,1
The Effects of Reward Misspecification Mapping and Mitigating Misaligned Models Reward hacking where RL agents exploit gaps in misspecified reward functions has been widely observed but not yet systematically studied. To understand how reward hacking arises we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities model capacity action space resolution observation space noise and training time. More capable agents often exploit reward misspecifications achieving higher proxy reward and lower true reward than less capable agents. Moreover we find instances of phase transitions capability thresholds at which the agent s behavior qualitatively shifts leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this we propose an anomaly detection task for aberrant policies and offer several baseline detectors.,1
PixMix Dreamlike Pictures Comprehensively Improve Safety Measures In real world applications of machine learning reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out of distribution OOD robustness prediction consistency resilience to adversaries calibrated uncertainty estimates and the ability to detect anomalous inputs. However improving performance towards these goals is often a balancing act that today s methods cannot achieve without sacrificing performance on other safety axes. For instance adversarial training improves adversarial robustness but sharply degrades other classifier performance metrics. Similarly strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals which outperforms numerous baselines is near Pareto optimal and roundly improves safety measures.,1
A geometric framework for outlier detection in high dimensional data Outlier or anomaly detection is an important task in data analysis. We discuss the problem from a geometrical perspective and provide a framework that exploits the metric structure of a data set. Our approach rests on the manifold assumption i.e. that the observed nominally high dimensional data lie on a much lower dimensional manifold and that this intrinsic structure can be inferred with manifold learning methods. We show that exploiting this structure significantly improves the detection of outlying observations in high dimensional data. We also suggest a novel mathematically precise and widely applicable distinction between distributional and structural outliers based on the geometry and topology of the data manifold that clarifies conceptual ambiguities prevalent throughout the literature. Our experiments focus on functional data as one class of structured high dimensional data but the framework we propose is completely general and we include image and graph data applications. Our results show that the outlier structure of high dimensional and non tabular data can be detected and visualized using manifold learning methods and quantified using standard outlier scoring methods applied to the manifold embedding vectors.,1
Fast AdvProp Adversarial Propagation AdvProp is an effective way to improve recognition models leveraging adversarial examples. Nonetheless AdvProp suffers from the extremely slow training speed mainly because a extra forward and backward passes are required for generating adversarial examples b both original samples and their adversarial counterparts are used for training i.e. times data . In this paper we introduce Fast AdvProp which aggressively revamps AdvProp s costly training components rendering the method nearly as cheap as the vanilla training. Specifically our modifications in Fast AdvProp are guided by the hypothesis that disentangled learning with adversarial examples is the key for performance improvements while other training recipes e.g. paired clean and adversarial training samples multi step adversarial attackers could be largely simplified.,2
Demystifying the Adversarial Robustness of Random Transformation Defenses Neural networks lack of robustness against attacks raises concerns in security sensitive settings such as autonomous vehicles. While many countermeasures may look promising only a few withstand rigorous evaluation. Defenses using random transformations RT have shown impressive results particularly BaRT Raff et al. on ImageNet. However this type of defense has not been rigorously evaluated leaving its robustness properties poorly understood. Their stochastic properties make evaluation more challenging and render many proposed attacks on deterministic models inapplicable. First we show that the BPDA attack Athalye et al. a used in BaRT s evaluation is ineffective and likely overestimates its robustness. We then attempt to construct the strongest possible RT defense through the informed selection of transformations and Bayesian optimization for tuning their parameters. Furthermore we create the strongest possible attack to evaluate our RT defense. Our new attack vastly outperforms the baseline reducing the accuracy by compared to the reduction by the commonly used EoT attack . times improvement . Our result indicates that the RT defense on the Imagenette dataset a ten class subset of ImageNet is not robust against adversarial examples. Extending the study further we use our new attack to adversarially train RT defense called AdvRT resulting in a large robustness gain. Code is available at,2
Models Out of Line A Fourier Lens on Distribution Shift Robustness Improving the accuracy of deep neural networks DNNs on out of distribution OOD data is critical to an acceptance of deep learning DL in real world applications. It has been observed that accuracies on in distribution ID versus OOD data follow a linear trend and models that outperform this baseline are exceptionally rare and referred to as effectively robust . Recently some promising approaches have been developed to improve OOD robustness model pruning data augmentation and ensembling or zero shot evaluating large pretrained models. However there still is no clear understanding of the conditions on OOD data and model properties that are required to observe effective robustness. We approach this issue by conducting a comprehensive empirical study of diverse approaches that are known to impact OOD robustness on a broad range of natural and synthetic distribution shifts of CIFAR and ImageNet. In particular we view the effective robustness puzzle through a Fourier lens and ask how spectral properties of both models and OOD data influence the corresponding effective robustness. We find this Fourier lens offers some insight into why certain robust models particularly those from the CLIP family achieve OOD robustness. However our analysis also makes clear that no known metric is consistently the best explanation or even a strong explanation of OOD robustness. Thus to aid future research into the OOD puzzle we address the gap in publicly available models with effective robustness by introducing a set of pretrained models RobustNets with varying levels of OOD robustness.,2
Probable Domain Generalization via Quantile Risk Minimization Domain generalization DG seeks predictors which perform well on unseen test distributions by leveraging labeled training data from multiple related distributions or domains. To achieve this the standard formulation optimizes for worst case performance over the set of all possible domains. However with worst case shifts very unlikely in practice this generally leads to overly conservative solutions. In fact a recent study found that no DG algorithm outperformed empirical risk minimization in terms of average performance. In this work we argue that DG is neither a worst case problem nor an average case problem but rather a probabilistic one. To this end we propose a probabilistic framework for DG which we call Probable Domain Generalization wherein our key idea is that distribution shifts seen during training should inform us of probable shifts at test time. To realize this we explicitly relate training and test domains as draws from the same underlying meta distribution and propose a new optimization problem Quantile Risk Minimization QRM which requires that predictors generalize with high probability. We then prove that QRM i produces predictors that generalize to new domains with a desired probability given sufficiently many domains and samples and ii recovers the causal predictor as the desired probability of generalization approaches one. In our experiments we introduce a more holistic quantile focused evaluation protocol for DG and show that our algorithms outperform state of the art baselines on real and synthetic data.,2
Smooth Reduce Leveraging Patches for Improved Certified Robustness Randomized smoothing RS has been shown to be a fast scalable technique for certifying the robustness of deep neural network classifiers. However methods based on RS require augmenting data with large amounts of noise which leads to significant drops in accuracy. We propose a training free modified smoothing approach Smooth Reduce that leverages patching and aggregation to provide improved classifier certificates. Our algorithm classifies overlapping patches extracted from an input image and aggregates the predicted logits to certify a larger radius around the input. We study two aggregation schemes max and mean and show that both approaches provide better certificates in terms of certified accuracy average certified radii and abstention rates as compared to concurrent approaches. We also provide theoretical guarantees for such certificates and empirically show significant improvements over other randomized smoothing methods that require expensive retraining. Further we extend our approach to videos and provide meaningful certificates for video classifiers. A project page can be found at,2
Obfuscated Gradients Give a False Sense of Security Circumventing Defenses to Adversarial Examples We identify obfuscated gradients a kind of gradient masking as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization based attacks we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect and for each of the three types of obfuscated gradients we discover we develop attack techniques to overcome it. In a case study examining non certified white box secure defenses at ICLR we find obfuscated gradients are a common occurrence with of defenses relying on obfuscated gradients. Our new attacks successfully circumvent completely and partially in the original threat model each paper considers.,2
Intrinsic dimension estimation for discrete metrics Real world datasets characterized by discrete features are ubiquitous from categorical surveys to clinical questionnaires from unweighted networks to DNA sequences. Nevertheless the most common unsupervised dimensional reduction methods are designed for continuous spaces and their use for discrete spaces can lead to errors and biases. In this letter we introduce an algorithm to infer the intrinsic dimension ID of datasets embedded in discrete spaces. We demonstrate its accuracy on benchmark datasets and we apply it to analyze a metagenomic dataset for species fingerprinting finding a surprisingly small ID of order . This suggests that evolutive pressure acts on a low dimensional manifold despite the high dimensionality of sequences space.,2
Data Augmentation Can Improve Robustness Adversarial training suffers from robust overfitting a phenomenon where the robust test accuracy starts to decrease during training. In this paper we focus on reducing robust overfitting by using common data augmentation schemes. We demonstrate that contrary to previous findings when combined with model weight averaging data augmentation can significantly boost robust accuracy. Furthermore we compare various augmentations techniques and observe that spatial composition techniques work the best for adversarial training. Finally we evaluate our approach on CIFAR against ell infty and ell norm bounded perturbations of size epsilon and epsilon respectively. We show large absolute improvements of . and . in robust accuracy compared to previous state of the art methods. In particular against ell infty norm bounded perturbations of size epsilon our model reaches . robust accuracy without using any external data. We also achieve a significant performance boost with this approach while using other architectures and datasets such as CIFAR SVHN and TinyImageNet.,2
Adversarial Text Normalization Text based adversarial attacks are becoming more commonplace and accessible to general internet users. As these attacks proliferate the need to address the gap in model robustness becomes imminent. While retraining on adversarial data may increase performance there remains an additional class of character level attacks on which these models falter. Additionally the process to retrain a model is time and resource intensive creating a need for a lightweight reusable defense. In this work we propose the Adversarial Text Normalizer a novel method that restores baseline performance on attacked content with low computational overhead. We evaluate the efficacy of the normalizer on two problem areas prone to adversarial attacks i.e. Hate Speech and Natural Language Inference. We find that text normalization provides a task agnostic defense against character level attacks that can be implemented supplementary to adversarial retraining solutions which are more suited for semantic alterations.,2
Formulating Robustness Against Unforeseen Attacks Existing defenses against adversarial examples such as adversarial training typically assume that the adversary will conform to a specific or known threat model such as ell p perturbations within a fixed budget. In this paper we focus on the scenario where there is a mismatch in the threat model assumed by the defense during training and the actual capabilities of the adversary at test time. We ask the question if the learner trains against a specific source threat model when can we expect robustness to generalize to a stronger unknown target threat model during test time Our key contribution is to formally define the problem of learning and generalization with an unforeseen adversary which helps us reason about the increase in adversarial risk from the conventional perspective of a known adversary. Applying our framework we derive a generalization bound which relates the generalization gap between source and target threat models to variation of the feature extractor which measures the expected maximum difference between extracted features across a given threat model. Based on our generalization bound we propose adversarial training with variation regularization AT VR which reduces variation of the feature extractor across the source threat model during training. We empirically demonstrate that AT VR can lead to improved generalization to unforeseen attacks during test time compared to standard adversarial training on Gaussian and image datasets.,2
Universal Adversarial Triggers for Attacking and Analyzing NLP Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers input agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient guided search over tokens which finds short trigger sequences e.g. one word for classification and four words for language modeling that successfully trigger the target prediction. For example triggers cause SNLI entailment accuracy to drop from . to . of why questions in SQuAD to be answered to kill american people and the GPT language model to spew racist output even when conditioned on non racial contexts. Furthermore although the triggers are optimized using white box access to a specific model they transfer to other models for all tasks we consider. Finally since triggers are input agnostic they provide an analysis of global model behavior. For instance they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.,2
Increasing Confidence in Adversarial Robustness Evaluations Hundreds of defenses have been proposed to make deep neural networks robust against minimal adversarial input perturbations. However only a handful of these defenses held up their claims because correctly evaluating robustness is extremely challenging Weak attacks often fail to find adversarial examples even if they unknowingly exist thereby making a vulnerable network look robust. In this paper we propose a test to identify weak attacks and thus weak defense evaluations. Our test slightly modifies a neural network to guarantee the existence of an adversarial example for every sample. Consequentially any correct attack must succeed in breaking this modified network. For eleven out of thirteen previously published defenses the original evaluation of the defense fails our test while stronger attacks that break these defenses pass it. We hope that attack unit tests such as ours will be a major component in future robustness evaluations and increase confidence in an empirical field that is currently riddled with skepticism.,2
Anomal E A Self Supervised Network Intrusion Detection System based on Graph Neural Networks This paper investigates Graph Neural Networks GNNs application for self supervised network intrusion and anomaly detection. GNNs are a deep learning approach for graph based data that incorporate graph structures into learning to generalise graph representations and output embeddings. As network flows are naturally graph based GNNs are a suitable fit for analysing and learning network behaviour. The majority of current implementations of GNN based Network Intrusion Detection Systems NIDSs rely heavily on labelled network traffic which can not only restrict the amount and structure of input traffic but also the NIDSs potential to adapt to unseen attacks. To overcome these restrictions we present Anomal E a GNN approach to intrusion and anomaly detection that leverages edge features and graph topological structure in a self supervised process. This approach is to the best our knowledge the first successful and practical approach to network intrusion detection that utilises network flows in a self supervised edge leveraging GNN. Experimental results on two modern benchmark NIDS datasets not only clearly display the improvement of using Anomal E embeddings rather than raw features but also the potential Anomal E has for detection on wild network traffic.,3
HierarchicalForecast A Reference Framework for Hierarchical Forecasting in Python Large collections of time series data are commonly organized into cross sectional structures with different levels of aggregation examples include product and geographical groupings. A necessary condition for coherent decision making and planning with such datasets is for the dis aggregated series forecasts to add up exactly to the aggregated series forecasts which motivates the creation of novel hierarchical forecasting algorithms. The growing interest of the Machine Learning community in cross sectional hierarchical forecasting systems states that we are in a propitious moment to ensure that scientific endeavors are grounded on sound baselines. For this reason we put forward the HierarchicalForecast library which contains preprocessed publicly available datasets evaluation metrics and a compiled set of statistical baseline models. Our Python based framework aims to bridge the gap between statistical econometric modeling and Machine Learning forecasting research. Code and documentation are available in,3
AnoShift A Distribution Shift Benchmark for Unsupervised Anomaly Detection Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning and to the best of our knowledge there is none for unsupervised learning. Therefore we introduce an unsupervised anomaly detection benchmark with data that shifts over time built over Kyoto a traffic dataset for network intrusion detection. This kind of data meets the premise of shifting the input distribution it covers a large time span years with naturally occurring changes over time eg users modifying their behavior patterns and software updates . We first highlight the non stationary nature of the data using a basic per feature analysis t SNE and an Optimal Transport approach for measuring the overall distribution distances between years. Next we propose AnoShift a protocol splitting the data in IID NEAR and FAR testing splits. We validate the performance degradation over time with diverse models MLM to classical Isolation Forest . Finally we show that by acknowledging the distribution shift problem and properly addressing it the performance can be improved compared to the classical IID training by up to on average . Dataset and code are available at,3
A State Transition Model for Mobile Notifications via Survival Analysis Mobile notifications have become a major communication channel for social networking services to keep users informed and engaged. As more mobile applications push notifications to users they constantly face decisions on what to send when and how. A lack of research and methodology commonly leads to heuristic decision making. Many notifications arrive at an inappropriate moment or introduce too many interruptions failing to provide value to users and spurring users complaints. In this paper we explore unique features of interactions between mobile notifications and user engagement. We propose a state transition framework to quantitatively evaluate the effectiveness of notifications. Within this framework we develop a survival model for badging notifications assuming a log linear structure and a Weibull distribution. Our results show that this model achieves more flexibility for applications and superior prediction accuracy than a logistic regression model. In particular we provide an online use case on notification delivery time optimization to show how we make better decisions drive more user engagement and provide more value to users.,4
Adversarial Sign Corrupted Isotonic Regression Classical univariate isotonic regression involves nonparametric estimation under a monotonicity constraint of the true signal. We consider a variation of this generating process which we term adversarial sign corrupted isotonic texttt ASCI regression. Under this texttt ASCI setting the adversary has full access to the true isotonic responses and is free to sign corrupt them. Estimating the true monotonic signal given these sign corrupted responses is a highly challenging task. Notably the sign corruptions are designed to violate monotonicity and possibly induce heavy dependence between the corrupted response terms. In this sense texttt ASCI regression may be viewed as an adversarial stress test for isotonic regression. Our motivation is driven by understanding whether efficient robust estimation of the monotone signal is feasible under this adversarial setting. We develop texttt ASCIFIT a three step estimation procedure under the texttt ASCI setting. The texttt ASCIFIT procedure is conceptually simple easy to implement with existing software and consists of applying the texttt PAVA with crucial pre and post processing corrections. We formalize this procedure and demonstrate its theoretical guarantees in the form of sharp high probability upper bounds and minimax lower bounds. We illustrate our findings with detailed simulations.,4
Variational Neural Networks Bayesian Neural Networks BNNs provide a tool to estimate the uncertainty of a neural network by considering a distribution over weights and sampling different models for each input. In this paper we propose a method for uncertainty estimation in neural networks called Variational Neural Network that instead of considering a distribution over weights generates parameters for the output distribution of a layer by transforming its inputs with learnable sub layers. In uncertainty quality estimation experiments we show that VNNs achieve better uncertainty quality than Monte Carlo Dropout or Bayes By Backpropagation methods.,4
Jackknife Variability Estimation For Randomized Matrix Computations Randomized algorithms based on sketching have become a workhorse tool in low rank matrix approximation. To use these algorithms safely in applications they should be coupled with diagnostics to assess the quality of approximation. To meet this need this paper proposes a jackknife resampling method to estimate the variability of the output of a randomized matrix computation. The variability estimate can recognize that a computation requires additional data or that the computation is intrinsically unstable. As examples the paper studies jackknife estimates for two randomized low rank matrix approximation algorithms. In each case the operation count for the jackknife estimate is independent of the dimensions of the target matrix. In numerical experiments the estimator accurately assesses variability and also provides an order of magnitude estimate of the mean square error.,4
Contextual Bandits with Smooth Regret Efficient Learning in Continuous Action Spaces Designing efficient general purpose contextual bandit algorithms that work with large or even continuous action spaces would facilitate application to important scenarios such as information retrieval recommendation systems and continuous control. While obtaining standard regret guarantees can be hopeless alternative regret notions have been proposed to tackle the large action setting. We propose a smooth regret notion for contextual bandits which dominates previously proposed alternatives. We design a statistically and computationally efficient algorithm for the proposed smooth regret that works with general function approximation under standard supervised oracles. We also present an adaptive algorithm that automatically adapts to any smoothness level. Our algorithms can be used to recover the previous minimax Pareto optimal guarantees under the standard regret e.g. in bandit problems with multiple best arms and Lipschitz H lder bandits. We conduct large scale empirical evaluations demonstrating the efficacy of our proposed algorithms.,4
Efficient Search of Multiple Neural Architectures with Different Complexities via Importance Sampling Neural architecture search NAS aims to automate architecture design processes and improve the performance of deep neural networks. Platform aware NAS methods consider both performance and complexity and can find well performing architectures with low computational resources. Although ordinary NAS methods result in tremendous computational costs owing to the repetition of model training one shot NAS which trains the weights of a supernetwork containing all candidate architectures only once during the search process has been reported to result in a lower search cost. This study focuses on the architecture complexity aware one shot NAS that optimizes the objective function composed of the weighted sum of two metrics such as the predictive performance and number of parameters. In existing methods the architecture search process must be run multiple times with different coefficients of the weighted sum to obtain multiple architectures with different complexities. This study aims at reducing the search cost associated with finding multiple architectures. The proposed method uses multiple distributions to generate architectures with different complexities and updates each distribution using the samples obtained from multiple distributions based on importance sampling. The proposed method allows us to obtain multiple architectures with different complexities in a single architecture search resulting in reducing the search cost. The proposed method is applied to the architecture search of convolutional neural networks on the CIAFR and ImageNet datasets. Consequently compared with baseline methods the proposed method finds multiple architectures with varying complexities while requiring less computational effort.,4
Inference of Regulatory Networks Through Temporally Sparse Data A major goal in genomics is to properly capture the complex dynamical behaviors of gene regulatory networks GRNs . This includes inferring the complex interactions between genes which can be used for a wide range of genomics analyses including diagnosis or prognosis of diseases and finding effective treatments for chronic diseases such as cancer. Boolean networks have emerged as a successful class of models for capturing the behavior of GRNs. In most practical settings inference of GRNs should be achieved through limited and temporally sparse genomics data. A large number of genes in GRNs leads to a large possible topology candidate space which often cannot be exhaustively searched due to the limitation in computational resources. This paper develops a scalable and efficient topology inference for GRNs using Bayesian optimization and kernel based methods. Rather than an exhaustive search over possible topologies the proposed method constructs a Gaussian Process GP with a topology inspired kernel function to account for correlation in the likelihood function. Then using the posterior distribution of the GP model the Bayesian optimization efficiently searches for the topology with the highest likelihood value by optimally balancing between exploration and exploitation. The performance of the proposed method is demonstrated through comprehensive numerical experiments using a well known mammalian cell cycle network.,4
Matching Normalizing Flows and Probability Paths on Manifolds Continuous Normalizing Flows CNFs are a class of generative models that transform a prior distribution to a model distribution by solving an ordinary differential equation ODE . We propose to train CNFs on manifolds by minimizing probability path divergence PPD a novel family of divergences between the probability density path generated by the CNF and a target probability density path. PPD is formulated using a logarithmic mass conservation formula which is a linear first order partial differential equation relating the log target probabilities and the CNF s defining vector field. PPD has several key benefits over existing methods it sidesteps the need to solve an ODE per iteration readily applies to manifold data scales to high dimensions and is compatible with a large family of target paths interpolating pure noise and data in finite time. Theoretically PPD is shown to bound classical probability divergences. Empirically we show that CNFs learned by minimizing PPD achieve state of the art results in likelihoods and sample quality on existing low dimensional manifold benchmarks and is the first example of a generative model to scale to moderately high dimensional manifolds.,4
Graph Neural Network Bandits We consider the bandit optimization problem with the reward function defined over graph structured data.,4
Nonparametric regression with modified ReLU networks We consider regression estimation with modified ReLU neural networks in which network weight matrices are first modified by a function alpha before being multiplied by input vectors. We give an example of continuous piecewise linear function alpha for which the empirical risk minimizers over the classes of modified ReLU networks with l and squared l penalties attain up to a logarithmic factor the minimax rate of prediction of unknown beta smooth function.,4
Is a Caption Worth a Thousand Images A Controlled Study for Representation Learning The development of CLIP Radford et al. has sparked a debate on whether language supervision can result in vision models with more transferable representations than traditional image only methods. Our work studies this question through a carefully controlled comparison of two approaches in terms of their ability to learn representations that generalize to downstream classification tasks. We find that when the pre training dataset meets certain criteria it is sufficiently large and contains descriptive captions with low variability image only methods do not match CLIP s transfer performance even when they are trained with more image data. However contrary to what one might expect there are practical settings in which these criteria are not met wherein added supervision through captions is actually detrimental. Motivated by our findings we devise simple prescriptions to enable CLIP to better leverage the language information present in existing pre training datasets.,4
Information Processing Equalities and the Information Risk Bridge We introduce two new classes of measures of information for statistical experiments which generalise and subsume phi divergences integral probability metrics mathfrak N distances MMD and f Gamma divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem thus extending the variational phi divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.,4
Characterizing the Effect of Class Imbalance on the Learning Dynamics Data imbalance is a common problem in the machine learning literature that can have a critical effect on the performance of a model. Various solutions exist such as the ones that focus on resampling or data generation but their impact on the convergence of gradient based optimizers used in deep learning is not understood. We here elucidate the significant negative impact of data imbalance on learning showing that the learning curves for minority and majority classes follow sub optimal trajectories when training with a gradient based optimizer. The reason is not only that the gradient signal neglects the minority classes but also that the minority classes are subject to a larger directional noise which slows their learning by an amount related to the imbalance ratio. To address this problem we propose a new algorithmic solution for which we provide a detailed analysis of its convergence behavior. We show both theoretically and empirically that this new algorithm exhibits a better behavior with more stable learning curves for each class as well as a better generalization performance.,4
Differentially Private Graph Learning via Sensitivity Bounded Personalized PageRank Personalized PageRank PPR is a fundamental tool in unsupervised learning of graph representations such as node ranking labeling and graph embedding. However while data privacy is one of the most important recent concerns existing PPR algorithms are not designed to protect user privacy. PPR is highly sensitive to the input graph edges the difference of only one edge may cause a big change in the PPR vector potentially leaking private user data.,4
Private Convex Optimization in General Norms We propose a new framework for differentially private optimization of convex functions which are Lipschitz in an arbitrary norm normx cdot . Our algorithms are based on a regularized exponential mechanism which samples from the density propto exp k F mu r where F is the empirical loss and r is a regularizer which is strongly convex with respect to normx cdot generalizing a recent work of cite GLL to non Euclidean settings. We show that this mechanism satisfies Gaussian differential privacy and solves both DP ERM empirical risk minimization and DP SCO stochastic convex optimization by using localization tools from convex geometry. Our framework is the first to apply to private convex optimization in general normed spaces and directly recovers non private SCO rates achieved by mirror descent as the privacy parameter eps to infty . As applications for Lipschitz optimization in ell p norms for all p in we obtain the first optimal privacy utility tradeoffs for p we improve tradeoffs obtained by the recent works cite AsiFKT BassilyGN by at least a logarithmic factor. Our ell p norm and Schatten p norm optimization frameworks are complemented with polynomial time samplers whose query complexity we explicitly bound.,4
Orthogonalization of data via Gromov Wasserstein type feedback for clustering and visualization In this paper we propose an adaptive approach for clustering and visualization of data by an orthogonalization process. Starting with the data points being represented by a Markov process using the diffusion map framework the method adaptively increase the orthogonality of the clusters by applying a feedback mechanism inspired by the Gromov Wasserstein distance. This mechanism iteratively increases the spectral gap and refines the orthogonality of the data to achieve a clustering with high specificity. By using the diffusion map framework and representing the relation between data points using transition probabilities the method is robust with respect to both the underlying distance noise in the data and random initialization. We prove that the method converges globally to a unique fixpoint for certain parameter values. We also propose a related approach where the transition probabilities in the Markov process are required to be doubly stochastic in which case the method generates a minimizer to a nonconvex optimization problem. We apply the method on cryo electron microscopy image data from biopharmaceutical manufacturing where we can confirm biologically relevant insights related to therapeutic efficacy. We consider an example with morphological variations of gene packaging and confirm that the method produces biologically meaningful clustering results consistent with human expert classification.,4
Optimal precision for GANs When learning disconnected distributions Generative adversarial networks GANs are known to face model misspecification. Indeed a continuous mapping from a unimodal latent distribution to a disconnected one is impossible so GANs necessarily generate samples outside of the support of the target distribution. This raises a fundamental question what is the latent space partition that minimizes the measure of these areas Building on a recent result of geometric measure theory we prove that an optimal GANs must structure its latent space as a simplicial cluster a Voronoi partition where cells are convex cones when the dimension of the latent space is larger than the number of modes. In this configuration each Voronoi cell maps to a distinct mode of the data. We derive both an upper and a lower bound on the optimal precision of GANs learning disconnected manifolds. Interestingly these two bounds have the same order of decrease sqrt log m m being the number of modes. Finally we perform several experiments to exhibit the geometry of the latent space and experimentally show that GANs have a geometry with similar properties to the theoretical one.,4
Mathematical Foundations of Graph Based Bayesian Semi Supervised Learning In recent decades science and engineering have been revolutionized by a momentous growth in the amount of available data. However despite the unprecedented ease with which data are now collected and stored labeling data by supplementing each feature with an informative tag remains to be challenging. Illustrative tasks where the labeling process requires expert knowledge or is tedious and time consuming include labeling X rays with a diagnosis protein sequences with a protein type texts by their topic tweets by their sentiment or videos by their genre. In these and numerous other examples only a few features may be manually labeled due to cost and time constraints. How can we best propagate label information from a small number of expensive labeled features to a vast number of unlabeled ones This is the question addressed by semi supervised learning SSL .,4
A Near Optimal Primal Dual Method for Off Policy Learning in CMDP As an important framework for safe Reinforcement Learning the Constrained Markov Decision Process CMDP has been extensively studied in the recent literature. However despite the rich results under various on policy learning settings there still lacks some essential understanding of the offline CMDP problems in terms of both the algorithm design and the information theoretic sample complexity lower bound. In this paper we focus on solving the CMDP problems where only offline data are available. By adopting the concept of the single policy concentrability coefficient C we establish an Omega left frac min left mathcal S mathcal A mathcal S I right C gamma epsilon right sample complexity lower bound for the offline CMDP problem where I stands for the number of constraints. By introducing a simple but novel deviation control mechanism we propose a near optimal primal dual learning algorithm called DPDL. This algorithm provably guarantees zero constraint violation and its sample complexity matches the above lower bound except for an tilde mathcal O gamma factor. Comprehensive discussion on how to deal with the unknown constant C and the potential asynchronous structure on the offline dataset are also included.,4
Ultra low latency recurrent neural network inference on FPGAs for physics applications with hls ml Recurrent neural networks have been shown to be effective architectures for many tasks in high energy physics and thus have been widely adopted. Their use in low latency environments has however been limited as a result of the difficulties of implementing recurrent architectures on field programmable gate arrays FPGAs . In this paper we present an implementation of two types of recurrent neural network layers long short term memory and gated recurrent unit within the hls ml framework. We demonstrate that our implementation is capable of producing effective designs for both small and large models and can be customized to meet specific design requirements for inference latencies and FPGA resources. We show the performance and synthesized designs for multiple neural networks many of which are trained specifically for jet identification tasks at the CERN Large Hadron Collider.,4
Fully Decentralized Model based Policy Optimization for Networked Systems Reinforcement learning algorithms require a large amount of samples this often limits their real world applications on even simple tasks. Such a challenge is more outstanding in multi agent tasks as each step of operation is more costly requiring communications or shifting or resources. This work aims to improve data efficiency of multi agent control by model based learning. We consider networked systems where agents are cooperative and communicate only locally with their neighbors and propose the decentralized model based policy optimization framework DMPO . In our method each agent learns a dynamic model to predict future states and broadcast their predictions by communication and then the policies are trained under the model rollouts. To alleviate the bias of model generated data we restrain the model usage for generating myopic rollouts thus reducing the compounding error of model generation. To pertain the independence of policy update we introduce extended value function and theoretically prove that the resulting policy gradient is a close approximation to true policy gradients. We evaluate our algorithm on several benchmarks for intelligent transportation systems which are connected autonomous vehicle control tasks Flow and CACC and adaptive traffic signal control ATSC . Empirically results show that our method achieves superior data efficiency and matches the performance of model free methods using true models.,4
Learning structures of the French clinical language development and validation of word embedding models using million clinical reports from electronic health records Background,4
Probabilistic forecasting for geosteering in fluvial successions using a generative adversarial network Quantitative workflows utilizing real time data to constrain ahead of bit uncertainty have the potential to improve geosteering significantly. Fast updates based on real time data are essential when drilling in complex reservoirs with high uncertainties in pre drill models. However practical assimilation of real time data requires effective geological modeling and mathematically robust parameterization. We propose a generative adversarial deep neural network GAN trained to reproduce geologically consistent D sections of fluvial successions. Offline training produces a fast GAN based approximation of complex geology parameterized as a dimensional model vector with standard Gaussian distribution of each component. Probabilistic forecasts are generated using an ensemble of equiprobable model vector realizations. A forward modeling sequence including a GAN converts the initial prior ensemble of realizations into EM log predictions. An ensemble smoother minimizes statistical misfits between predictions and real time data yielding an update of model vectors and reduced uncertainty around the well. Updates can be then translated to probabilistic predictions of facies and resistivities. The present paper demonstrates a workflow for geosteering in an outcrop based synthetic fluvial succession. In our example the method reduces uncertainty and correctly predicts most major geological features up to meters ahead of drill bit.,4
Local manifold learning and its link to domain based physics knowledge In many reacting flow systems the thermo chemical state space is known or assumed to evolve close to a low dimensional manifold LDM . Various approaches are available to obtain those manifolds and subsequently express the original high dimensional space with fewer parameterizing variables. Principal component analysis PCA is one of the dimensionality reduction methods that can be used to obtain LDMs. PCA does not make prior assumptions about the parameterizing variables and retrieves them empirically from the training data. In this paper we show that PCA applied in local clusters of data local PCA is capable of detecting the intrinsic parameterization of the thermo chemical state space. We first demonstrate that utilizing three common combustion models of varying complexity the Burke Schumann model the chemical equilibrium model and the homogeneous reactor. Parameterization of these models is known a priori which allows for benchmarking with the local PCA approach. We further extend the application of local PCA to a more challenging case of a turbulent non premixed n heptane air jet flame for which the parameterization is no longer obvious. Our results suggest that meaningful parameterization can be obtained also for more complex datasets. We show that local PCA finds variables that can be linked to local stoichiometry reaction progress and soot formation processes.,4
Goal Conditioned Generators of Deep Policies Goal conditioned Reinforcement Learning RL aims at learning optimal policies given goals encoded in special command inputs. Here we study goal conditioned neural nets NNs that learn to generate deep NN policies in form of context specific weight matrices similar to Fast Weight Programmers and other methods from the s. Using context commands of the form generate a policy that achieves a desired expected return our NN generators combine powerful exploration of parameter space with generalization across commands to iteratively find better and better policies. A form of weight sharing HyperNetworks and policy embeddings scales our method to generate deep NNs. Experiments show how a single learned policy generator can produce policies that achieve any return seen during training. Finally we evaluate our algorithm on a set of continuous control tasks where it exhibits competitive performance. Our code is public.,4
A Certifiable Security Patch for Object Tracking in Self Driving Systems via Historical Deviation Modeling Self driving cars SDC commonly implement the perception pipeline to detect the surrounding obstacles and track their moving trajectories which lays the ground for the subsequent driving decision making process. Although the security of obstacle detection in SDC is intensively studied not until very recently the attackers start to exploit the vulnerability of the tracking module. Compared with solely attacking the object detectors this new attack strategy influences the driving decision more effectively with less attack budgets. However little is known on whether the revealed vulnerability remains effective in end to end self driving systems and if so how to mitigate the threat.,4
When does SGD favor flat minima A quantitative characterization via linear stability The observation that stochastic gradient descent SGD favors flat minima has played a fundamental role in understanding implicit regularization of SGD and guiding the tuning of hyperparameters. In this paper we provide a quantitative explanation of this striking phenomenon by relating the particular noise structure of SGD to its emph linear stability Wu et al. . Specifically we consider training over parameterized models with square loss. We prove that if a global minimum theta is linearly stable for SGD then it must satisfy H theta F leq O sqrt B eta where H theta F B eta denote the Frobenius norm of Hessian at theta batch size and learning rate respectively. Otherwise SGD will escape from that minimum emph exponentially fast. Hence for minima accessible to SGD the flatness as measured by the Frobenius norm of the Hessian is bounded independently of the model size and sample size. The key to obtaining these results is exploiting the particular geometry awareness of SGD noise the noise magnitude is proportional to loss value the noise directions concentrate in the sharp directions of local landscape. This property of SGD noise provably holds for linear networks and random feature models RFMs and is empirically verified for nonlinear networks. Moreover the validity and practical relevance of our theoretical findings are justified by extensive numerical experiments.,4
Statistical Inference with Stochastic Gradient Algorithms Stochastic gradient algorithms are widely used for both optimization and sampling in large scale learning and inference problems. However in practice tuning these algorithms is typically done using heuristics and trial and error rather than rigorous generalizable theory. To address this gap between theory and practice we novel insights into the effect of tuning parameters by characterizing the large sample behavior of iterates of a very general class of preconditioned stochastic gradient algorithms with fixed step size. In the optimization setting our results show that iterate averaging with a large fixed step size can result in statistically efficient approximation of the local M estimator. In the sampling context our results show that with appropriate choices of tuning parameters the limiting stationary covariance can match either the Bernstein von Mises limit of the posterior adjustments to the posterior for model misspecification or the asymptotic distribution of the MLE and that with a naive tuning the limit corresponds to none of these. Moreover we argue that an essentially independent sample from the stationary distribution can be obtained after a fixed number of passes over the dataset. We validate our asymptotic results in realistic finite sample regimes via several experiments using simulated and real data. Overall we demonstrate that properly tuned stochastic gradient algorithms with constant step size offer a computationally efficient and statistically robust approach to obtaining point estimates or posterior like samples.,4
Learning Optimal Transport Between two Empirical Distributions with Normalizing Flows Optimal transport OT provides effective tools for comparing and mapping probability measures. We propose to leverage the flexibility of neural networks to learn an approximate optimal transport map. More precisely we present a new and original method to address the problem of transporting a finite set of samples associated with a first underlying unknown distribution towards another finite set of samples drawn from another unknown distribution. We show that a particular instance of invertible neural networks namely the normalizing flows can be used to approximate the solution of this OT problem between a pair of empirical distributions. To this aim we propose to relax the Monge formulation of OT by replacing the equality constraint on the push forward measure by the minimization of the corresponding Wasserstein distance. The push forward operator to be retrieved is then restricted to be a normalizing flow which is trained by optimizing the resulting cost function. This approach allows the transport map to be discretized as a composition of functions. Each of these functions is associated to one sub flow of the network whose output provides intermediate steps of the transport between the original and target measures. This discretization yields also a set of intermediate barycenters between the two measures of interest. Experiments conducted on toy examples as well as a challenging task of unsupervised translation demonstrate the interest of the proposed method. Finally some experiments show that the proposed approach leads to a good approximation of the true OT.,4
Deeply Learned Generalized Linear Models with Missing Data Deep Learning DL methods have dramatically increased in popularity in recent years with significant growth in their application to supervised learning problems in the biomedical sciences. However the greater prevalence and complexity of missing data in modern biomedical datasets present significant challenges for DL methods. Here we provide a formal treatment of missing data in the context of deeply learned generalized linear models a supervised DL architecture for regression and classification problems. We propose a new architecture textit dlglm that is one of the first to be able to flexibly account for both ignorable and non ignorable patterns of missingness in input features and response at training time. We demonstrate through statistical simulation that our method outperforms existing approaches for supervised learning tasks in the presence of missing not at random MNAR missingness. We conclude with a case study of a Bank Marketing dataset from the UCI Machine Learning Repository in which we predict whether clients subscribed to a product based on phone survey data.,4
Online Active Regression Active regression considers a linear regression problem where the learner receives a large number of data points but can only observe a small number of labels. Since online algorithms can deal with incremental training data and take advantage of low computational cost we consider an online extension of the active regression problem the learner receives data points one by one and immediately decides whether it should collect the corresponding labels. The goal is to efficiently maintain the regression of received data points with a small budget of label queries. We propose novel algorithms for this problem under ell p loss where p in . To achieve a epsilon approximate solution our proposed algorithms only require tilde mathcal O epsilon d log n kappa queries of labels where n is the number of data points and kappa is a quantity called the condition number of the data points. The numerical results verify our theoretical results and show that our methods have comparable performance with offline active regression algorithms.,4
Uncertainty aware Mixed variable Machine Learning for Materials Design Data driven design shows the promise of accelerating materials discovery but is challenging due to the prohibitive cost of searching the vast design space of chemistry structure and synthesis methods. Bayesian Optimization BO employs uncertainty aware machine learning models to select promising designs to evaluate hence reducing the cost. However BO with mixed numerical and categorical variables which is of particular interest in materials design has not been well studied. In this work we survey frequentist and Bayesian approaches to uncertainty quantification of machine learning with mixed variables. We then conduct a systematic comparative study of their performances in BO using a popular representative model from each group the random forest based Lolo model frequentist and the latent variable Gaussian process model Bayesian . We examine the efficacy of the two models in the optimization of mathematical functions as well as properties of structural and functional materials where we observe performance differences as related to problem dimensionality and complexity. By investigating the machine learning models predictive and uncertainty estimation capabilities we provide interpretations of the observed performance differences. Our results provide practical guidance on choosing between frequentist and Bayesian uncertainty aware machine learning models for mixed variable BO in materials design.,4
An Asymmetric Contrastive Loss for Handling Imbalanced Datasets Contrastive learning is a representation learning method performed by contrasting a sample to other similar samples so that they are brought closely together forming clusters in the feature space. The learning process is typically conducted using a two stage training architecture and it utilizes the contrastive loss CL for its feature learning. Contrastive learning has been shown to be quite successful in handling imbalanced datasets in which some classes are overrepresented while some others are underrepresented. However previous studies have not specifically modified CL for imbalanced datasets. In this work we introduce an asymmetric version of CL referred to as ACL in order to directly address the problem of class imbalance. In addition we propose the asymmetric focal contrastive loss AFCL as a further generalization of both ACL and focal contrastive loss FCL . Results on the FMNIST and ISIC imbalanced datasets show that AFCL is capable of outperforming CL and FCL in terms of both weighted and unweighted classification accuracies. In the appendix we provide a full axiomatic treatment on entropy along with complete proofs.,4
Single Model Uncertainty Estimation via Stochastic Data Centering We are interested in estimating the uncertainties of deep neural networks which play an important role in many scientific and engineering problems. In this paper we present a striking new finding that an ensemble of neural networks with the same weight initialization trained on datasets that are shifted by a constant bias gives rise to slightly inconsistent trained models where the differences in predictions are a strong indicator of epistemic uncertainties. Using the neural tangent kernel NTK we demonstrate that this phenomena occurs in part because the NTK is not shift invariant. Since this is achieved via a trivial input transformation we show that it can therefore be approximated using just a single neural network using a technique that we call Delta UQ that estimates uncertainty around prediction by marginalizing out the effect of the biases. We show that Delta UQ s uncertainty estimates are superior to many of the current methods on a variety of benchmarks outlier rejection calibration under distribution shift and sequential design optimization of black box functions.,4
Estimation of Non Crossing Quantile Regression Process with Deep ReQU Neural Networks We propose a penalized nonparametric approach to estimating the quantile regression process QRP in a nonseparable model using rectifier quadratic unit ReQU activated deep neural networks and introduce a novel penalty function to enforce non crossing of quantile regression curves. We establish the non asymptotic excess risk bounds for the estimated QRP and derive the mean integrated squared error for the estimated QRP under mild smoothness and regularity conditions. To establish these non asymptotic risk and estimation error bounds we also develop a new error bound for approximating C s smooth functions with s and their derivatives using ReQU activated neural networks. This is a new approximation result for ReQU networks and is of independent interest and may be useful in other problems. Our numerical experiments demonstrate that the proposed method is competitive with or outperforms two existing methods including methods using reproducing kernels and random forests for nonparametric quantile regression.,4
AGBoost Attention based Modification of Gradient Boosting Machine A new attention based model for the gradient boosting machine GBM called AGBoost the attention based gradient boosting is proposed for solving regression problems. The main idea behind the proposed AGBoost model is to assign attention weights with trainable parameters to iterations of GBM under condition that decision trees are base learners in GBM. Attention weights are determined by applying properties of decision trees and by using the Huber s contamination model which provides an interesting linear dependence between trainable parameters of the attention and the attention weights. This peculiarity allows us to train the attention weights by solving the standard quadratic optimization problem with linear constraints. The attention weights also depend on the discount factor as a tuning parameter which determines how much the impact of the weight is decreased with the number of iterations. Numerical experiments performed for two types of base learners original decision trees and extremely randomized trees with various regression datasets illustrate the proposed model.,4
Uncertainty quantification for predictions of atomistic neural networks The value of uncertainty quantification on predictions for trained neural networks NNs on quantum chemical reference data is quantitatively explored. For this the architecture of the PhysNet NN was suitably modified and the resulting model was evaluated with different metrics to quantify calibration quality of predictions and whether prediction error and the predicted uncertainty can be correlated. The results from training on the QM database and evaluating data from the test set within and outside the distribution indicate that error and uncertainty are not linearly related. The results clarify that noise and redundancy complicate property prediction for molecules even in cases for which changes e.g. double bond migration in two otherwise identical molecules are small. The model was then applied to a real database of tautomerization reactions. Analysis of the distance between members in feature space combined with other parameters shows that redundant information in the training dataset can lead to large variances and small errors whereas the presence of similar but unspecific information returns large errors but small variances. This was e.g. observed for nitro containing aliphatic chains for which predictions were difficult although the training set contained several examples for nitro groups bound to aromatic molecules. This underlines the importance of the composition of the training data and provides chemical insight into how this affects the prediction capabilities of a ML model. Finally the approach put forward can be used for information based improvement of chemical databases for target applications through active learning optimization.,4
Forget me not Contrastive Critics for Mitigating Posterior Collapse Variational autoencoders VAEs suffer from posterior collapse where the powerful neural networks used for modeling and inference optimize the objective without meaningfully using the latent representation. We introduce inference critics that detect and incentivize against posterior collapse by requiring correspondence between latent variables and the observations. By connecting the critic s objective to the literature in self supervised contrastive representation learning we show both theoretically and empirically that optimizing inference critics increases the mutual information between observations and latents mitigating posterior collapse. This approach is straightforward to implement and requires significantly less training time than prior methods yet obtains competitive results on three established datasets. Overall the approach lays the foundation to bridge the previously disconnected frameworks of contrastive learning and probabilistic modeling with variational autoencoders underscoring the benefits both communities may find at their intersection.,4
A Generative Framework for Personalized Learning and Estimation Theory Algorithms and Privacy A distinguishing characteristic of federated learning is that the local client data could have statistical heterogeneity. This heterogeneity has motivated the design of personalized learning where individual personalized models are trained through collaboration. There have been various personalization methods proposed in literature with seemingly very different forms and methods ranging from use of a single global model for local regularization and model interpolation to use of multiple global models for personalized clustering etc. In this work we begin with a generative framework that could potentially unify several different algorithms as well as suggest new algorithms. We apply our generative framework to personalized estimation and connect it to the classical empirical Bayes methodology. We develop private personalized estimation under this framework. We then use our generative framework for learning which unifies several known personalized FL algorithms and also suggests new ones we propose and study a new algorithm AdaPeD based on a Knowledge Distillation which numerically outperforms several known algorithms. We also develop privacy for personalized learning methods with guarantees for user level privacy and composition. We numerically evaluate the performance as well as the privacy for both the estimation and learning problems demonstrating the advantages of our proposed methods.,4
Rewiring Networks for Graph Neural Network Training Using Discrete Geometry Information over squashing is a phenomenon of inefficient information propagation between distant nodes on networks. It is an important problem that is known to significantly impact the training of graph neural networks GNNs as the receptive field of a node grows exponentially. To mitigate this problem a preprocessing procedure known as rewiring is often applied to the input network. In this paper we investigate the use of discrete analogues of classical geometric notions of curvature to model information flow on networks and rewire them. We show that these classical notions achieve state of the art performance in GNN training accuracy on a variety of real world network datasets. Moreover compared to the current state of the art these classical notions exhibit a clear advantage in computational runtime by several orders of magnitude.,4
The role of the geometric mean in case control studies Historically used in settings where the outcome is rare or data collection is expensive outcome dependent sampling is relevant to many modern settings where data is readily available for a biased sample of the target population such as public administrative data. Under outcome dependent sampling common effect measures such as the average risk difference and the average risk ratio are not identified but the conditional odds ratio is. Aggregation of the conditional odds ratio is challenging since summary measures are generally not identified. Furthermore the marginal odds ratio can be larger or smaller than all conditional odds ratios. This so called non collapsibility of the odds ratio is avoidable if we use an alternative aggregation to the standard arithmetic mean. We provide a new definition of collapsibility that makes this choice of aggregation method explicit and we demonstrate that the odds ratio is collapsible under geometric aggregation. We describe how to partially identify estimate and do inference on the geometric odds ratio under outcome dependent sampling. Our proposed estimator is based on the efficient influence function and therefore has doubly robust style properties.,4
The derivatives of Sinkhorn Knopp converge We show that the derivatives of the Sinkhorn Knopp algorithm or iterative proportional fitting procedure converge towards the derivatives of the entropic regularization of the optimal transport problem with a locally uniform linear convergence rate.,4
AMLB an AutoML Benchmark Comparing different AutoML frameworks is notoriously challenging and often done incorrectly. We introduce an open and extensible benchmark that follows best practices and avoids common mistakes when comparing AutoML frameworks. We conduct a thorough comparison of well known AutoML frameworks across classification and regression tasks. The differences between the AutoML frameworks are explored with a multi faceted analysis evaluating model accuracy its trade offs with inference time and framework failures. We also use Bradley Terry trees to discover subsets of tasks where the relative AutoML framework rankings differ. The benchmark comes with an open source tool that integrates with many AutoML frameworks and automates the empirical evaluation process end to end from framework installation and resource allocation to in depth evaluation. The benchmark uses public data sets can be easily extended with other AutoML frameworks and tasks and has a website with up to date results.,4
Improved conformalized quantile regression Conformalized quantile regression is a procedure that inherits the advantages of conformal prediction and quantile regression. That is we use quantile regression to estimate the true conditional quantile and then apply a conformal step on a calibration set to ensure marginal coverage. In this way we get adaptive prediction intervals that account for heteroscedasticity. However the aforementioned conformal step lacks adaptiveness as described in Romano et al. . To overcome this limitation instead of applying a single conformal step after estimating conditional quantiles with quantile regression we propose to cluster the explanatory variables weighted by their permutation importance with an optimized k means and apply k conformal steps. To show that this improved version outperforms the classic version of conformalized quantile regression and is more adaptive to heteroscedasticity we extensively compare the prediction intervals of both in open datasets.,4
Grounding Aleatoric Uncertainty in Unsupervised Environment Design Adaptive curricula in reinforcement learning RL have proven effective for producing policies robust to discrepancies between the train and test environment. Recently the Unsupervised Environment Design UED framework generalized RL curricula to generating sequences of entire environments leading to new methods with robust minimax regret properties. Problematically in partially observable or stochastic settings optimal policies may depend on the ground truth distribution over aleatoric parameters of the environment in the intended deployment setting while curriculum learning necessarily shifts the training distribution. We formalize this phenomenon as curriculum induced covariate shift CICS and describe how its occurrence in aleatoric parameters can lead to suboptimal policies. Directly sampling these parameters from the ground truth distribution avoids the issue but thwarts curriculum learning. We propose SAMPLR a minimax regret UED method that optimizes the ground truth utility function even when the underlying training data is biased due to CICS. We prove and validate on challenging domains that our approach preserves optimality under the ground truth distribution while promoting robustness across the full range of environment settings.,4
Learning Bellman Complete Representations for Offline Policy Evaluation We study representation learning for Offline Reinforcement Learning RL focusing on the important task of Offline Policy Evaluation OPE . Recent work shows that in contrast to supervised learning realizability of the Q function is not enough for learning it. Two sufficient conditions for sample efficient OPE are Bellman completeness and coverage. Prior work often assumes that representations satisfying these conditions are given with results being mostly theoretical in nature. In this work we propose BCRL which directly learns from data an approximately linear Bellman complete representation with good coverage. With this learned representation we perform OPE using Least Square Policy Evaluation LSPE with linear functions in our learned representation. We present an end to end theoretical analysis showing that our two stage algorithm enjoys polynomial sample complexity provided some representation in the rich class considered is linear Bellman complete. Empirically we extensively evaluate our algorithm on challenging image based continuous control tasks from the Deepmind Control Suite. We show our representation enables better OPE compared to previous representation learning methods developed for off policy RL e.g. CURL SPR . BCRL achieve competitive OPE error with the state of the art method Fitted Q Evaluation FQE and beats FQE when evaluating beyond the initial state distribution. Our ablations show that both linear Bellman complete and coverage components of our method are crucial.,4
Benign Tempered or Catastrophic A Taxonomy of Overfitting The practical success of overparameterized neural networks has motivated the recent scientific study of interpolating methods which perfectly fit their training data. Certain interpolating methods including neural networks can fit noisy training data without catastrophically bad test performance in defiance of standard intuitions from statistical learning theory. Aiming to explain this a body of recent work has studied textit benign overfitting a phenomenon where some interpolating methods approach Bayes optimality even in the presence of noise. In this work we argue that while benign overfitting has been instructive and fruitful to study many real interpolating methods like neural networks textit do not fit benignly modest noise in the training set causes nonzero but non infinite excess risk at test time implying these models are neither benign nor catastrophic but rather fall in an intermediate regime. We call this intermediate regime textit tempered overfitting and we initiate its systematic study. We first explore this phenomenon in the context of kernel ridge regression KR by obtaining conditions on the ridge parameter and kernel eigenspectrum under which KR exhibits each of the three behaviors. We find that kernels with powerlaw spectra including Laplace kernels and ReLU neural tangent kernels exhibit tempered overfitting. We then empirically study deep neural networks through the lens of our taxonomy and find that those trained to interpolation are tempered while those stopped early are benign. We hope our work leads to a more refined understanding of overfitting in modern learning.,4
High dimensional stochastic linear contextual bandit with missing covariates Recent works in bandit problems adopted lasso convergence theory in the sequential decision making setting. Even with fully observed contexts there are technical challenges that hinder the application of existing lasso convergence theory proving the restricted eigenvalue condition under conditionally sub Gaussian noise and accounting for the dependence between the context variables and the chosen actions. This paper studies the effect of missing covariates on regret for stochastic linear bandit algorithms. Our work provides a high probability upper bound on the regret incurred by the proposed algorithm in terms of covariate sampling probabilities showing that the regret degrades due to missingness by at most zeta min where zeta min is the minimum probability of observing covariates in the context vector. We illustrate our algorithm for the practical application of experimental design for collecting gene expression data by a sequential selection of class discriminating DNA probes.,4
Exploration in Linear Bandits with Rich Action Sets and its Implications for Inference We present a non asymptotic lower bound on the eigenspectrum of the design matrix generated by any linear bandit algorithm with sub linear regret when the action set has well behaved curvature. Specifically we show that the minimum eigenvalue of the expected design matrix grows as Omega sqrt n whenever the expected cumulative regret of the algorithm is O sqrt n where n is the learning horizon and the action space has a constant Hessian around the optimal arm. This shows that such action spaces force a polynomial lower bound rather than a logarithmic lower bound as shown by cite lattimore end in discrete i.e. well separated action spaces. Furthermore while the previous result is shown to hold only in the asymptotic regime as n to infty our result for these locally rich action spaces is any time. Additionally under a mild technical assumption we obtain a similar lower bound on the minimum eigen value holding with high probability.,4
Future Dependent Value Based Off Policy Evaluation in POMDPs We study off policy evaluation OPE for partially observable MDPs POMDPs with general function approximation. Existing methods such as sequential importance sampling estimators and fitted Q evaluation suffer from the curse of horizon in POMDPs. To circumvent this problem we develop a novel model free OPE method by introducing future dependent value functions that take future proxies as inputs. Future dependent value functions play similar roles as classical value functions in fully observable MDPs. We derive a new Bellman equation for future dependent value functions as conditional moment equations that use history proxies as instrumental variables. We further propose a minimax learning method to learn future dependent value functions using the new Bellman equation. We obtain the PAC result which implies our OPE estimator is consistent as long as futures and histories contain sufficient information about latent states and the Bellman completeness. Finally we extend our methods to learning of dynamics and establish the connection between our approach and the well known spectral learning methods in POMDPs.,4
A clinically motivated self supervised approach for content based image retrieval of CT liver images Deep learning based approaches for content based image retrieval CBIR of CT liver images is an active field of research but suffers from some critical limitations. First they are heavily reliant on labeled data which can be challenging and costly to acquire. Second they lack transparency and explainability which limits the trustworthiness of deep CBIR systems. We address these limitations by proposing a self supervised learning framework that incorporates domain knowledge into the training procedure and providing the first representation learning explainability analysis in the context of CBIR of CT liver images. Results demonstrate improved performance compared to the standard self supervised approach across several metrics as well as improved generalisation across datasets. Further we conduct the first representation learning explainability analysis in the context of CBIR which reveals new insights into the feature extraction process. Lastly we perform a case study with cross examination CBIR that demonstrates the usability of our proposed framework. We believe that our proposed framework could play a vital role in creating trustworthy deep CBIR systems that can successfully take advantage of unlabeled data.,4
Neural Stein critics with staged L regularization Learning to differentiate model distributions from observed data is a fundamental problem in statistics and machine learning and high dimensional data remains a challenging setting for such problems. Metrics that quantify the disparity in probability distributions such as the Stein discrepancy play an important role in statistical testing in high dimensions. In this paper we consider the setting where one wishes to distinguish between data sampled from an unknown probability distribution and a nominal model distribution. While recent studies revealed that the optimal L regularized Stein critic equals the difference of the score functions of two probability distributions up to a multiplicative constant we investigate the role of L regularization when training a neural network Stein discrepancy critic function. Motivated by the Neural Tangent Kernel theory of training neural networks we develop a novel staging procedure for the weight of regularization over training time. This leverages the advantages of highly regularized training at early times while also empirically delaying overfitting. Theoretically we relate the training dynamic with large regularization weight to the kernel regression optimization of lazy training regime in early training times. The benefit of the staged L regularization is demonstrated on simulated high dimensional distribution drift data and an application to evaluating generative models of image data.,4
Riemannian Diffusion Schr dinger Bridge Score based generative models exhibit state of the art performance on density estimation and generative modeling tasks. These models typically assume that the data geometry is flat yet recent extensions have been developed to synthesize data living on Riemannian manifolds. Existing methods to accelerate sampling of diffusion models are typically not applicable in the Riemannian setting and Riemannian score based methods have not yet been adapted to the important task of interpolation of datasets. To overcome these issues we introduce emph Riemannian Diffusion Schr dinger Bridge . Our proposed method generalizes Diffusion Schr dinger Bridge introduced in cite debortoli neurips to the non Euclidean setting and extends Riemannian score based models beyond the first time reversal. We validate our proposed method on synthetic data and real Earth and climate data.,4
Off the grid learning of sparse mixtures from a continuous dictionary We consider a general non linear model where the signal is a finite mixture of an unknown possibly increasing number of features issued from a continuous dictionary parameterized by a real nonlinear parameter. The signal is observed with Gaussian possibly correlated noise in either a continuous or a discrete setup. We propose an off the grid optimization method that is a method which does not use any discretization scheme on the parameter space to estimate both the non linear parameters of the features and the linear parameters of the mixture. We use recent results on the geometry of off the grid methods to give minimal separation on the true underlying non linear parameters such that interpolating certificate functions can be constructed. Using also tail bounds for suprema of Gaussian processes we bound the prediction error with high probability. Assuming that the certificate functions can be constructed our prediction error bound is up to log factors similar to the rates attained by the Lasso predictor in the linear regression model. We also establish convergence rates that quantify with high probability the quality of estimation for both the linear and the non linear parameters.,4
Intrinsic dimension estimation for discrete metrics Real world datasets characterized by discrete features are ubiquitous from categorical surveys to clinical questionnaires from unweighted networks to DNA sequences. Nevertheless the most common unsupervised dimensional reduction methods are designed for continuous spaces and their use for discrete spaces can lead to errors and biases. In this letter we introduce an algorithm to infer the intrinsic dimension ID of datasets embedded in discrete spaces. We demonstrate its accuracy on benchmark datasets and we apply it to analyze a metagenomic dataset for species fingerprinting finding a surprisingly small ID of order . This suggests that evolutive pressure acts on a low dimensional manifold despite the high dimensionality of sequences space.,4
Mean field Variational Inference via Wasserstein Gradient Flow Variational inference VI provides an appealing alternative to traditional sampling based approaches for implementing Bayesian inference due to its conceptual simplicity statistical accuracy and computational scalability. However common variational approximation schemes such as the mean field MF approximation require certain conjugacy structure to facilitate efficient computation which may add unnecessary restrictions to the viable prior distribution family and impose further constraints on the variational approximation family. In this work we develop a general computational framework for implementing MF VI via Wasserstein gradient flow WGF a gradient flow over the space of probability measures. When specialized to Bayesian latent variable models we analyze the algorithmic convergence of an alternating minimization scheme based on a time discretized WGF for implementing the MF approximation. In particular the proposed algorithm resembles a distributional version of EM algorithm consisting of an E step of updating the latent variable variational distribution and an M step of conducting steepest descent over the variational distribution of parameters. Our theoretical analysis relies on optimal transport theory and subdifferential calculus in the space of probability measures. We prove the exponential convergence of the time discretized WGF for minimizing a generic objective functional given strict convexity along generalized geodesics. We also provide a new proof of the exponential contraction of the variational distribution obtained from the MF approximation by using the fixed point equation of the time discretized WGF. We apply our method and theory to two classic Bayesian latent variable models the Gaussian mixture model and the mixture of regression model. Numerical experiments are also conducted to compliment the theoretical findings under these two models.,4
